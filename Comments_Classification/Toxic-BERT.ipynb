{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Классификация комментариев с BERT и TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Цели:\n",
    "* Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "* Постройте модель со значением метрики качества *F1* не меньше 0.75. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "from tqdm import notebook\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModel, AutoTokenizer \n",
    "\n",
    "from catboost import CatBoostClassifier, Pool, cv\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('popular', quiet=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:0.3f}'.format\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка комментариев."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://code.s3.yandex.net/datasets/toxic_comments.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценим сбалансированность выборки.\n",
    "\n",
    "* Токсичных комментариев намного меньше.\n",
    "* Присутствует дисбаланс классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   0.898\n",
       "1   0.102\n",
       "Name: toxic, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toxic.value_counts() / df.toxic.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очистим текст при помощи регулярных выражений. Это впоследствии пригодится для tf-idf с CatBoost и поможет провести проверку на дубликаты. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ограничим число символов - 800. Этого достаточно для задачи и поможет увеличить скорость расчетов.\n",
    "def clear_text(text, char_limit=800):\n",
    "    # Приведем текст к нижнему регистру очистив его от лишних символов и цифр.\n",
    "    text = re.sub(r\"[^a-z']\", ' ', text.lower())\n",
    "    \n",
    "    # Обрежим сообщение до ближайшего целого слова.\n",
    "    try:\n",
    "        r = re.match(r'(?P<truncated> .+) \\s', text[:char_limit], re.X)\n",
    "        text = r['truncated'].split()\n",
    "        return ' '.join(text)\n",
    "    except:\n",
    "        return 'empty line'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    explanation why the edits made under my userna...\n",
       "1    d'aww he matches this background colour i'm se...\n",
       "2    hey man i'm really not trying to edit war it's...\n",
       "3    more i can't make any real suggestions on impr...\n",
       "4    you sir are my hero any chance you remember wh...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = df['text'].map(clear_text).copy()\n",
    "text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь проверим дубликаты и удалим их."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1570"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(158001, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[~text.duplicated()]\n",
    "text.drop_duplicates(inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Датафрейм для tf-idf с CatBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = pd.concat([text, df['toxic']], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обработаем текст при помощи лемматизатора Wordnet из NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализируем WordNetLemmatizer.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Функция для определения формы речи перед лемматизацией.\n",
    "def get_wordnet_pos(word):\n",
    "    # nltk.pos_tag() возвращает кортеж с тегом POS.\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# Функция лемматизации.\n",
    "def lemmatize(text):\n",
    "    return ' '.join(\n",
    "        [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(text)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лемматизация процесс долгий. Воспользуемся pickle для сохранения и загрузки её результатов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('lemmatized_test.pickle', 'rb') as f:\n",
    "        text = pickle.load(f)\n",
    "except:\n",
    "    text = text.map(lemmatize)\n",
    "    with open('lemmatized_test.pickle', 'wb') as f:\n",
    "        pickle.dump(text, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведем последовательное обучением различных моделей для получения более подробного отчета."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## CatBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим выборку и сформируем Pool'ы для CatBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    text_df['text'],\n",
    "    text_df['toxic'],\n",
    "    test_size=0.2,\n",
    "    stratify=text_df['toxic'],\n",
    "    random_state=38)\n",
    "\n",
    "# Преобразуем в DataFrame чтобы можно было выбрать text_features в CatBoost Pool.\n",
    "X_train = pd.DataFrame(X_train)\n",
    "X_test = pd.DataFrame(X_test)\n",
    "train_set = Pool(X_train, y_train, text_features=['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведем кросс-валидацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Гиперпараметры.\n",
    "params = {\n",
    "    'objective': 'Logloss',\n",
    "    'loss_function': 'Logloss',\n",
    "    'depth': 6,\n",
    "    'subsample': 0.8,\n",
    "    'random_strength': 0.3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iterations           999.000\n",
       "test-Logloss-mean      0.120\n",
       "test-Logloss-std       0.003\n",
       "train-Logloss-mean     0.114\n",
       "train-Logloss-std      0.001\n",
       "Name: 999, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_data = cv(\n",
    "    pool=train_set,\n",
    "    params=params,\n",
    "    early_stopping_rounds=100,\n",
    "    logging_level='Silent',\n",
    "    fold_count=4,\n",
    "    stratified=True,\n",
    "    partition_random_seed=38)\n",
    "\n",
    "# Результаты кросс-валидации.\n",
    "cv_data.iloc[-1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим метрики на тестовой выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.081354\n",
      "0:\tlearn: 0.5742707\ttotal: 57.5ms\tremaining: 57.4s\n",
      "250:\tlearn: 0.1199002\ttotal: 20.6s\tremaining: 1m 1s\n",
      "500:\tlearn: 0.1102371\ttotal: 40.4s\tremaining: 40.2s\n",
      "750:\tlearn: 0.1030989\ttotal: 1m\tremaining: 20s\n",
      "999:\tlearn: 0.0976653\ttotal: 1m 19s\tremaining: 0us\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98     28388\n",
      "           1       0.86      0.68      0.76      3213\n",
      "\n",
      "    accuracy                           0.96     31601\n",
      "   macro avg       0.91      0.83      0.87     31601\n",
      "weighted avg       0.95      0.96      0.95     31601\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cb = CatBoostClassifier(random_state=38, **params)\n",
    "cb.fit(train_set, verbose=250)\n",
    "\n",
    "print()\n",
    "print(classification_report(y_test, cb.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(158001, 6000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Подгрузим стоп слова.\n",
    "nltk_stopwords = stopwords.words('english')\n",
    "# Векторизация TF-IDF текстовых данных.\n",
    "vectorizer = TfidfVectorizer(max_df=0.8, # дает лучше результат чем nltk_stopwords\n",
    "                             max_features=6000)\n",
    "\n",
    "X = vectorizer.fit_transform(text)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(126400, 6000) (31601, 6000)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, df.loc[text.index, 'toxic'],\n",
    "    test_size=0.2,\n",
    "    stratify=df.loc[text.index, 'toxic'],\n",
    "    random_state=38)\n",
    "\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем основные метрики полученные у LogisticRegression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98     28388\n",
      "           1       0.80      0.75      0.78      3213\n",
      "\n",
      "    accuracy                           0.96     31601\n",
      "   macro avg       0.89      0.86      0.88     31601\n",
      "weighted avg       0.95      0.96      0.96     31601\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(max_iter=500,\n",
    "                        class_weight={0: 1, 1: 2},\n",
    "                        n_jobs=-1,\n",
    "                        C=5,\n",
    "                        random_state=38)\n",
    "lr.fit(X_train, y_train)\n",
    "print(classification_report(y_test, lr.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT - unitary/toxic-bert model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем использовать AutoModel и AutoTokenizer из библиотеки transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at unitary/toxic-bert were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Зададим модель, она как раз заточена под toxic comments.\n",
    "model_name = 'unitary/toxic-bert'\n",
    "\n",
    "# Загрузим предобученную PyTorch модель.\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отредактируем настройки, в частности поменяем problem_type с мультиклассовой классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config = model.config.from_dict(\n",
    "    {\n",
    "        'id2label': {'0': 'not_toxic', '1': 'toxic'},\n",
    "        'label2id': {'not_toxic': '0', 'toxic': '1'},\n",
    "        'problem_type': 'single_label_classification'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем данные. На выходе получим input tokens и маску значимости."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, _, attention_mask = tokenizer(df['text'].values.tolist(),\n",
    "                                         return_tensors='pt',\n",
    "                                         max_length=256, # ограничим длину, иначе всё очень долго считается.\n",
    "                                         padding=True, \n",
    "                                         truncation=True).values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сформируем окончательные признаки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Попробуем загрузить фичи.\n",
    "try:\n",
    "    with open('features_full.pickle', 'rb') as f:\n",
    "        features = pickle.load(f)    \n",
    "except:\n",
    "    # Количесто батчей чтобы не забить всю память расчетами.\n",
    "    batch_size = 100\n",
    "    size = df.shape[0]\n",
    "    embeddings = []\n",
    "\n",
    "    for i in notebook.tqdm(range(size // batch_size + (size % batch_size != 0))):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids[batch_size*i:batch_size*(i+1)],\n",
    "                            attention_mask=attention_mask[batch_size*i:batch_size*(i+1)])\n",
    "            # Найдем среднее в выходном тензоре.\n",
    "            outputs_mean = torch.mean(outputs[0], dim=1)\n",
    "            embeddings.append(outputs_mean)\n",
    "\n",
    "    with open('features_full.pickle', 'wb') as f:\n",
    "        pickle.dump(features, f)\n",
    "        \n",
    "    features = np.concatenate(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(158001, 768)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбивка выборок."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(features, df['toxic'], test_size=0.2, random_state=38, stratify=df['toxic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предсказание модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99     28388\n",
      "           1       0.94      0.95      0.94      3213\n",
      "\n",
      "    accuracy                           0.99     31601\n",
      "   macro avg       0.97      0.97      0.97     31601\n",
      "weighted avg       0.99      0.99      0.99     31601\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(n_jobs=-1)\n",
    "lr.fit(X_train, y_train)\n",
    "print(classification_report(y_test, lr.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выводы\n",
    "\n",
    "* Получена модель, классифицирующая комментарии на позитивные и негативные.\n",
    "* Достигнута f1 ~ 0.94 на тестовой выборке благодаря правильно выбранной модели unitary/toxic-bert.\n",
    "* Проведено сравнение BERT, CatBoostClassifier и TF-IDF.\n",
    "    * Если доступно много ресурсов и нужна повышенная точность, то лучше всего использовать трасформеры.\n",
    "    * Если требуется быстрый результат из коробки, то побеждает CatBoostClassifier.\n",
    "* Векторизация и последующая классификация TF-IDF показала себя неплохо, но возможно следует выбрать другой классификатор для достижения более высоких результатов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
